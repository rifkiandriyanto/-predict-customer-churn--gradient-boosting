{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5929113,"sourceType":"datasetVersion","datasetId":3404076}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n\n# Load training and testing datasets\ntrain_data = pd.read_csv('/kaggle/input/customer-churn-dataset/customer_churn_dataset-training-master.csv')\ntest_data = pd.read_csv('/kaggle/input/customer-churn-dataset/customer_churn_dataset-testing-master.csv')\n\n# Specify the important columns\nimportant_columns = ['Age', 'Gender', 'Tenure', 'Usage Frequency', 'Churn']\n\n# View the first few rows of the important columns in training and testing data before preprocessing\nprint(\"Training Data - Important Columns (Before Preprocessing):\")\nprint(train_data[important_columns].head())\n\nprint(\"\\nTesting Data - Important Columns (Before Preprocessing):\")\nprint(test_data[important_columns].head())\n\n# Preprocessing the data\ndef preprocess_data(df):\n    # Encoding categorical columns\n    label_encoders = {}\n    categorical_columns = ['Gender', 'Subscription Type', 'Contract Length']\n    \n    for col in categorical_columns:\n        label_encoders[col] = LabelEncoder()\n        df[col] = label_encoders[col].fit_transform(df[col])\n\n    # Drop unnecessary columns\n    df = df.drop(columns=['CustomerID'])  # Drop CustomerID since it's not useful for prediction\n\n    # Handle missing values\n    df = df.fillna(df.median())  # Fill missing values with median values for numeric columns\n    \n    return df\n\n# Preprocess training and testing data\ntrain_data = preprocess_data(train_data)\ntest_data = preprocess_data(test_data)\n\n# Ensure there are no missing values in the target variable (Churn)\ntrain_data = train_data.dropna(subset=['Churn'])\ntest_data = test_data.dropna(subset=['Churn'])\n\n# View the first few rows of the training and testing data after preprocessing\nprint(\"\\nTraining Data - After Preprocessing:\")\nprint(train_data[important_columns].head())\n\nprint(\"\\nTesting Data - After Preprocessing:\")\nprint(test_data[important_columns].head())\n\n# Splitting features (X) and target variable (y)\nX_train = train_data.drop(columns=['Churn'])\ny_train = train_data['Churn']\n\nX_test = test_data.drop(columns=['Churn'])\ny_test = test_data['Churn']\n\n# Ensure target variable is numeric and has no missing values\ny_train = y_train.astype(float)\ny_test = y_test.astype(float)\n\n# Convert data to DMatrix format for XGBoost\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# Define XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',  # Binary classification\n    'eval_metric': 'auc',  # AUC-ROC metric\n    'learning_rate': 0.1,\n    'max_depth': 6,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n#     'n_estimators': 100,\n}\n\n# Train the model\nbst = xgb.train(params, dtrain, num_boost_round=100)\n\n# Make predictions\ny_pred_proba = bst.predict(dtest)\ny_pred = [1 if prob > 0.5 else 0 for prob in y_pred_proba]\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nauc_score = roc_auc_score(y_test, y_pred_proba)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'AUC Score: {auc_score:.4f}')\nprint('Classification Report:')\nprint(classification_rep)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-11T03:35:43.919845Z","iopub.execute_input":"2024-10-11T03:35:43.920866Z","iopub.status.idle":"2024-10-11T03:35:48.384249Z","shell.execute_reply.started":"2024-10-11T03:35:43.920818Z","shell.execute_reply":"2024-10-11T03:35:48.382989Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Training Data - Important Columns (Before Preprocessing):\n    Age  Gender  Tenure  Usage Frequency  Churn\n0  30.0  Female    39.0             14.0    1.0\n1  65.0  Female    49.0              1.0    1.0\n2  55.0  Female    14.0              4.0    1.0\n3  58.0    Male    38.0             21.0    1.0\n4  23.0    Male    32.0             20.0    1.0\n\nTesting Data - Important Columns (Before Preprocessing):\n   Age  Gender  Tenure  Usage Frequency  Churn\n0   22  Female      25               14      1\n1   41  Female      28               28      0\n2   47    Male      27               10      0\n3   35    Male       9               12      0\n4   53  Female      58               24      0\n\nTraining Data - After Preprocessing:\n    Age  Gender  Tenure  Usage Frequency  Churn\n0  30.0       0    39.0             14.0    1.0\n1  65.0       0    49.0              1.0    1.0\n2  55.0       0    14.0              4.0    1.0\n3  58.0       1    38.0             21.0    1.0\n4  23.0       1    32.0             20.0    1.0\n\nTesting Data - After Preprocessing:\n   Age  Gender  Tenure  Usage Frequency  Churn\n0   22       0      25               14      1\n1   41       0      28               28      0\n2   47       1      27               10      0\n3   35       1       9               12      0\n4   53       0      58               24      0\nAccuracy: 0.5036\nAUC Score: 0.7281\nClassification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.98      0.06      0.11     33881\n         1.0       0.49      1.00      0.66     30493\n\n    accuracy                           0.50     64374\n   macro avg       0.73      0.53      0.38     64374\nweighted avg       0.75      0.50      0.37     64374\n\n","output_type":"stream"}]}]}